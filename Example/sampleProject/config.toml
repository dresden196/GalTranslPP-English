[backendSpecific.OpenAI-Compatible]
apiStrategy = "random" # API key strategy: random=random rotation, fallback=prefer first key, use next on [request error]
apiTimeout = 120 # Request timeout in seconds

  [[backendSpecific.OpenAI-Compatible.apis]]
  apikey = ""             # sk-example-key | can be empty for Sakura engine
  apiurl = ""             # https://openai-example.com | request URL, with or without /v1
  modelName = ""          # modelName | can be empty for Sakura engine
  stream = false
  # enable = false
  # temperature = 1.0
  # topP = 1.0
  # frequencyPenalty = 0.0
  # presencePenalty = 0.0
  # extraKeys = [ "sk-extra-1", "sk-extra-2" ]

[plugins]
filePlugin = "NormalJson" # Supported formats: NormalJson, Epub, PDF, MtoolJson(not yet implemented)...
baseClassName = "NormalJson" # Same as above, specifies base class for custom filePlugin scripts to inherit

# ForGalJson: Feed JSON-formatted dialogue to AI, require JSON response. Works with any targetLang.
# ForGalTsv: Feed TSV-formatted dialogue to AI, require TSV response. Works with any targetLang. More token efficient.
# ForNovelTsv: Like ForGalTsv but without name in input. Works with any targetLang.
# Sakura: [CHINESE ONLY!] Uses local Sakura model for Japanese->Chinese translation. Does NOT support English output!
# DumpName: Generate NameTable.toml for name replacement
# GenDict: Auto-generate terminology glossary using AI. Uses GENDIC_SYSTEM and GENDIC_PROMPT prompts
# Rebuild: Skip retranslation even if problem or orig_text contains retranslKey, only rebuild from cache
# ShowNormal: Output preprocessed content and sentences. For EPUB, generates preprocessed html/xhtml and json for debugging
# Note: If project folder has Prompt.toml, it takes priority over BaseConfig
transEngine = "ForGalTsv"

#"SkipTrans"
# Text pre-processing plugin list, can set multiple, executed in order
# Pre/Post Plugins can use lua/py scripts as custom extension plugins
# Plugin format: "path/to/script.lua" / "path/to/script.py"
# e.g. "Lua/MySampleTextPlugin.lua"
# Plugins must define init and run functions, see README for examples
textPrePlugins = [
]

#"TextLinebreakFix"
#"TextPostFull2Half"
# Text post-processing plugin list, can set multiple, executed in order
textPostPlugins = [
]

[plugins.NormalJson]
output_with_src = true

[plugins.Epub]
"bilingual" = true

[[plugins.Epub.preprocRegex]]
org = '<ruby><rb>(.+?)</rb><rt>(.+?)</rt></ruby>'
rep = '[$2/$1]'

[[plugins.Epub.preprocRegex]]
org = '(<p[^>/]*>)(.*?)(</p>)'
callback = [
  { org = '<[^>]*>', rep = '', group = 2 },
]

[[plugins.Epub.postprocRegex]]


[plugins.SkipTrans]
skipH = false
# Actually copies original to translation and marks as translated, still affects context cache hits
# Syntax identical to retranslKeys
skipKeys = [
  '^[a-zA-Z\d_\p{P}]+$', # Equivalent to [{ conditionTarget = 'preproc_text', conditionReg = '^[a-zA-Z\d_\p{P}]+$' }]
]

[plugins.TextLinebreakFix]
"linebreakMode" = "preferPunctuation" # Reference for line count comparison is preproc_text (with <br>) not orig_text
"preferThreshold" = 0.24

[plugins.TextPostFull2Half]
"reverseReplace" = false  # false=fullwidth->halfwidth, true=halfwidth->fullwidth

[common]
numPerRequestTranslate = 8 # Sentences per request, recommended < 15
threadsNum = 5             # Max thread count
sortMethod = "size"        # Translation order: name=by filename, size=larger files first (improves multi-thread speed) [name/size]
targetLang = "en"          # Target language: en/zh-cn/zh-tw/ja/ko/ru/fr and more
splitFile = "No"           # Single file splitting: Num=split every n sentences, Equal=split each file into n parts, No=disable [No/Num/Equal]
splitFileNum = 10          # For Num: sentences per split; For Equal: parts per file
saveCacheInterval = 1      # Save cache every n translation batches
linebreakSymbol = "auto"   # Line break symbol used in this project's json
maxRetries = 5             # Max retry attempts
contextHistorySize = 8     # Number of context sentences to include
smartRetry = false         # On parse failure, try halving content and clearing context (not recommended for unofficial API keys)
checkQuota = true          # Dynamically check key quota at runtime, auto-remove exhausted keys from API pool
logLevel = "info"
saveLog = true

# Tokenizer backend for dict generation and error checking (except MeCab, others depend on Python, so slower/higher memory is normal)
# MeCab is Japanese-only; use other tokenizers for other languages with appropriate models
tokenizerBackend = "MeCab" # MeCab, spaCy, Stanza
# MeCab tokenizer dictionary path
mecabDictDir = "BaseConfig/mecabDict/mecab-ipadic-utf8" # Can also use mecab-ipadic-neologd or unidic (download from web)
# spaCy (Python) model name, see https://spacy.io/models, restart required after downloading new model
spaCyModelName = "ja_core_news_lg" # sm models aren't great, use trf models if possible
# Stanza uses LangID for models, see https://stanfordnlp.github.io/stanza/ner_models.html
stanzaLang = "ja"


[problemAnalyze]
# Problem checklist
problemList = [
  "High Word Frequency",
  "Punctuation Errors",

  "Lost Line Break",
  "Extra Line Break",
  #"Longer Than Source",
  #"Strictly Longer Than Source",
  "Dict Unused",

  "Remaining Japanese",
  #"Introduced Latin",
  #"Invalid Character",
  #"Language Mismatch",
  "Introduced Korean",
  "Introduced Traditional Chinese",
]

# Regex list - if any problem in cache matches, retranslate
# Specify conditionTarget and conditionReg via inline table to retranslate based on specific source/target text
# Can also use conditionScript and conditionFunc for external lua/py scripts (conditionFunc receives Sentence, returns bool)
# <PROJECT_DIR> macro represents current project path
# Prefix conditionTarget with prev_ or next_ for previous/next sentence, e.g., prev_prev_orig_text for two sentences back
retranslKeys = [
  #[{ conditionScript='<PROJECT_DIR>/Lua/MySampleTextPlugin.lua',conditionFunc='funcName'},
  #{ conditionScript='<PROJECT_DIR>/Python/MySampleTextPlugin.py',conditionFunc='funcName'}],
  #"Remaining Japanese",
  "Translation Failed", # Equivalent to [{ conditionTarget = 'problems', conditionReg = 'Translation Failed' }]
]

# Regex list - if a problem matches, don't add to problems list
# Specify conditionTarget and conditionReg via inline table for specific source/target conditions
# First item in array is the problem to ignore (also regex)
skipProblems = [
  # "^Introduced Latin: Live$"  # No conditions
  # If source contains 'モチモチ' and translation contains 'chewy', ignore problems matching 'Introduced Latin'
  [ 'Introduced Latin', { conditionTarget = 'preproc_text', conditionReg = 'モチモチ'},
    { conditionTarget = 'trans_preview', conditionReg = 'chewy'}],
]

# Punctuation marks to check for punctuation errors
punctSet = """（()）：:*[]{}<>『』「」"";；'/\\"""
codePage = "gbk"   # Code page for invalid character checking
langProbability = 0.94 # Language detection confidence (0-1), higher=more precise but may miss, lower=opposite

# Problem comparison base and check objects (defaults to orig_text and trans_preview)
# e.g., { base = 'orig_text', check = 'trans_preview', problemKey = 'Strictly Longer Than Source' }
# means when trans_preview is strictly longer than orig_text, record the problem
overwriteCompareObj = [
  { problemKey = "High Word Frequency", base = "orig_text", check = "trans_preview" },
  { problemKey = "Punctuation Errors", base = "orig_text", check = "trans_preview" },
  { problemKey = "Introduced Latin", base = "preproc_text", check = "trans_preview" },
  { problemKey = "Introduced Korean", base = "preproc_text", check = "trans_preview" },
  { problemKey = "Lost Line Break", base = "orig_text", check = "trans_preview" },
  { problemKey = "Extra Line Break", base = "orig_text", check = "trans_preview" },
  { problemKey = "Dict Unused", base = "preproc_text", check = "trans_preview" },
  { problemKey = "Language Mismatch", base = "preproc_text", check = "trans_preview" },
]

# See documentation for translation execution order
[dictionary]
# If duplicate dict filenames exist, project folder takes priority
# Otherwise reads from corresponding pre/gpt/post folders in defaultDictFolder
defaultDictFolder = "BaseConfig/Dict"
usePreDictInName = false   # Apply pre-dict to name field, sends replaced name to translation engine [true/false]
usePostDictInName = false  # Apply post-dict to name field [true/false]
usePreDictInMsg = true     # Apply pre-dict to message field, sends replaced message to translation engine [true/false]
usePostDictInMsg = true    # Apply post-dict to message field [true/false]

useGPTDictToReplaceName = false   # Apply GPT dict to name field (direct search-replace) [true/false]

preDict = [
  "Project PreDict.toml"
]
gptDict = [
  "Project GptDict.toml",
  "Project GptDict-Generated.toml"
]
postDict = [
  "Project PostDict.toml"
]
