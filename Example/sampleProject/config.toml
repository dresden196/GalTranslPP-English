[backendSpecific.OpenAI-Compatible]
apiStrategy = "random" # 令牌策略，random随机轮询，fallback优先第一个，出现[请求错误]时使用下一个
apiTimeout = 120 # 请求超时时间，单位秒

  [[backendSpecific.OpenAI-Compatible.apis]]
  apikey = ""             # sk-example-key | sakura引擎可不填
  apiurl = ""             # https://openai-example.com | 请求地址，加不加/v1都行
  modelName = ""          # modelName | sakura引擎可不填
  stream = false

[plugins]
filePlugin = "NormalJson" # 用于支持更多格式: NormalJson, Epub, PDF, MtoolJson(尚未实现)...
baseClassName = "NormalJson" # 参数同上，用于在使用自定义 filePlugin 时指定脚本要继承的基类

# ForGalJson: 向AI输入json格式的对话并要求AI以json格式回复，使用 FORGALJSON_SYSTEM 和 FORGALJSON_TRANS_PROMPT_EN 作为提示词
# ForGalTsv: 向AI输入TSV格式的对话并要求AI以TSV格式回复，使用 FORGALTSV_SYSTEM 和 FORGALTSV_TRANS_PROMPT_EN 作为提示词
# ForNovelTsv: 和 ForGalTsv 的区别是输入不带name，使用 FORNOVELTSV_SYSTEM 和 FORNOVELTSV_TRANS_PROMPT_EN 作为提示词
# Sakura: 使用 SAKURA_SYSTEM_PROMPT 和 SAKURA_TRANS_PROMPT 作为提示词
# DumpName: 生成 人名替换表.toml 以供替换人名
# GenDict: 借助AI自动生成术语表，使用 GENDIC_SYSTEM 和 GENDIC_PROMPT 作为提示词
# Rebuild: 即使problem或orig_text中包含retranslKey也不会重翻，只根据缓存翻译重建结果
# ShowNormal: 展示预处理后的内容及句子，如Epub格式下可生成预处理后的html/xhtml文件以及生成的json，可用于检查和排错
# PS: 如果项目文件夹下有 Prompt.toml，会优先使用项目文件夹下的提示词，否则再去 BaseConfig 下寻找
transEngine = "ForGalJson"

#"SkipTrans"
# 文本译前处理插件列表，可以设置多个，按顺序执行
# 译前/译后 Plugins 均可使用 lua/py 脚本作为自定义扩展插件
# 插件加载格式为 "path/to/script.lua" / "path/to/script.py"
# 如"Lua/MySampleTextPlugin.lua"
# 插件中必须定义 init 和 run 函数，具体示例详见 README
textPrePlugins = [
]

#"TextLinebreakFix"
#"TextPostFull2Half"
# 文本译后处理插件列表，可以设置多个，按顺序执行
textPostPlugins = [
] 

[plugins.NormalJson]
output_with_src = true

[plugins.Epub]
"双语显示" = true

[[plugins.Epub.preprocRegex]]
org = '<ruby><rb>(.+?)</rb><rt>(.+?)</rt></ruby>'
rep = '[$2/$1]'

[[plugins.Epub.preprocRegex]]
org = '(<p[^>/]*>)(.*?)(</p>)'
callback = [
  { org = '<[^>]*>', rep = '', group = 2 },
]

[[plugins.Epub.postprocRegex]]
org = '\[([^/\[\]]+?)/([^/\[\]]+?)\]'
rep = '<ruby><rb>$2</rb><rt>$1</rt></ruby>'

[plugins.SkipTrans]
skipH = false
# 实际上只是将原文复制给译文并标记为已翻译，所以依旧影响上下文的缓存命中
# 语法与 retranslKeys 完全相同
skipKeys = [
  '^[a-zA-Z\d_]+$', # 等效于 [{ conditionTarget = 'preproc_text', conditionReg = '^[a-zA-Z\d_]+$' }]
]

[plugins.TextLinebreakFix]
"换行模式" = "优先标点" # 判断换行数目是否相同的基准是 preproc_text(中的<br>) 而不是 orig_text，请注意
"优先阈值" = 0.24

[plugins.TextPostFull2Half]
"是否反向替换" = false  # false=全角->半角，true=半角->全角

[common]
numPerRequestTranslate = 8 # 单次请求翻译句子数量，推荐值 < 15
threadsNum = 5             # 最大线程数
sortMethod = "size"         # 翻译顺序，name为文件名，size为大文件优先，多线程时大文件优先可以提高整体速度[name/size]
targetLang = "zh-cn"        # 翻译到的目标语言，包括但不限于[zh-cn/zh-tw/en/ja/ko/ru/fr]
splitFile = "No"            # 是否启用单文件分割。Num: 每n条分割一次，Equal: 每个文件均分n份，No: 关闭单文件分割。[No/Num/Equal]
splitFileNum = 10            # Num时，表示n句拆分一次；Equal时，表示每个文件均分拆成n部分
saveCacheInterval = 1       # 每翻译n次保存一次缓存
linebreakSymbol = "auto"    # 这个项目在json中使用的换行符
maxRetries = 5              # 最大重试次数
contextHistorySize = 8      # 携带上文数量
smartRetry = false          # 解析结果失败时尝试折半重翻与清空上下文，如果用的打野 key 其实不建议开这个
checkQuota = true           # 运行时动态检测key额度，自动从 API 池中删除额度不足的 key
logLevel = "info"
saveLog = true

# 用于生成字典和查错的分词器后端及其设置 (除了MeCab，剩下的都依赖Python，所以速度变慢或内存占用变大是正常的)
# 但 MeCab 只限于日文分词，如果想分词其它语言需要使用别的分词器并指定对应语言的模型
tokenizerBackend = "MeCab" # MeCab, spaCy, Stanza
# MeCab 分词器所用的词典的路径
mecabDictDir = "BaseConfig/mecabDict/mecab-ipadic-utf8" # 也可以用 mecab-ipadic-neologd 或 unidic(均需从网上下载字典)
# spaCy(Python) 分词器所用的模型名，可从 https://spacy.io/models 查询，自动下载需重启程序
spaCyModelName = "ja_core_news_sm" # sm模型的效果有点一言难尽，有条件的建议上trf模型
# stanza 使用 LangID 来获取模型, 语言ID列表 https://stanfordnlp.github.io/stanza/ner_models.html
stanzaLang = "ja"


[problemAnalyze]
# 要发现的问题清单
problemList = [
  #"词频过高",
  "标点错漏",

  "丢失换行",
  "多加换行",
  #"比原文长",
  #"比原文长严格",
  "字典未使用",

  "残留日文",
  #"引入拉丁字母",
  #"非法字符",
  #"语言不通",
  "引入韩文",
  "引入繁体字",
]

# 正则表达式列表，如果句子缓存中的某条 problem 能被以下任一正则 search 通过，则进行重翻
# 如果想对指定原文/译文进行重翻，请通过内联表(数组)指定 conditionTarget 和 conditionReg
# 也可以指定 conditionScript 和 conditionFunc 来外接 lua/py 脚本(conditionFunc 接收 Sentence，返回bool)
# <PROJECT_DIR>为代表当前项目路径字符串的宏
# conditionTarget 加前缀 prev_ 或 next_ 可表示 前/后 句，如 prev_prev_orig_text 表示上上句原文，如果不存在则条件失败
retranslKeys = [
  #[{ conditionScript='<PROJECT_DIR>/Lua/MySampleTextPlugin.lua',conditionFunc='funcName'},
  #{ conditionScript='<PROJECT_DIR>/Python/MySampleTextPlugin.py',conditionFunc='funcName'}],
  #"残留日文",
  "翻译失败", # 等效于 [{ conditionTarget = 'problems', conditionReg = '翻译失败' }]
]

# 正则表达式列表，如果一条 problem 能被以下任一正则 search 通过，则不加入 problems 列表
# 如果想忽略指定原文/译文的指定问题，请通过内联表(数组)指定 conditionTarget 和 conditionReg
# 并在表数组第一项填入要忽略的 problem (同样也是正则表达式)
skipProblems = [
  # "^引入拉丁字母: Live$"  # 不加任何条件
  # 如果 原文中包含'モチモチ'且译文中包含'Q弹'，则忽略此句子中所有能被 '引入拉丁字母' search 通过的 problem
  [ '引入拉丁字母', { conditionTarget = 'preproc_text', conditionReg = 'モチモチ'},
    { conditionTarget = 'trans_preview', conditionReg = 'Q弹'}],
]

#规定标点错漏要查哪些标点
punctSet = """（()）：:*[]{}<>『』「」“”;；'/\\"""
codePage = "gbk"   # 非法字符要检查的字符集
langProbability = 0.94 # 语言不通检测的语言置信度，设置越高则检测越精准，但可能遗漏，反之亦然

# 问题的比较对象和被比较对象(不写则默认为 orig_text 和 trans_preview )
# 如 { base = 'orig_text', check = 'trans_preview', problemKey = '比原文长严格' }
# 意思是当 trans_preview 比 orig_text 严格长时，在 problem 中留下对应的问题。
overwriteCompareObj = [
  { problemKey = "词频过高", base = "orig_text", check = "trans_preview" },
  { problemKey = "标点错漏", base = "orig_text", check = "trans_preview" },
  { problemKey = "引入拉丁字母", base = "preproc_text", check = "trans_preview" },
  { problemKey = "引入韩文", base = "preproc_text", check = "trans_preview" },
  { problemKey = "丢失换行", base = "orig_text", check = "trans_preview" },
  { problemKey = "多加换行", base = "orig_text", check = "trans_preview" },
  { problemKey = "字典未使用", base = "preproc_text", check = "trans_preview" },
  { problemKey = "语言不通", base = "preproc_text", check = "trans_preview" },
]

# 翻译执行顺序详见使用说明
[dictionary]
# 如果出现同名字典文件，则只读取项目文件夹中的
# 否则读取 defaultDictFolder 中对应 pre/gpt/post 文件夹下的字典
defaultDictFolder = "BaseConfig/Dict" 
usePreDictInName = false   # 将译前字典用在name字段，可用于翻译name字段，会发送给翻译引擎替换后的name[true/false]
usePostDictInName = false  # 将译后字典用在name字段，可用于翻译name字段[true/false]
usePreDictInMsg = true   # 将译前字典用在message字段，可用于翻译message字段，会发送给翻译引擎替换后的message[true/false]
usePostDictInMsg = true  # 将译后字典用在message字段，可用于翻译message字段[true/false]

useGPTDictToReplaceName = false   # 将GPT字典用在name字段，可用于翻译name字段(直接搜索替换)[true/false]

preDict = [
  "项目译前字典.toml"
]
gptDict = [
  "项目GPT字典.toml",
  "项目GPT字典-生成.toml"
]
postDict = [
  "项目译后字典.toml"
]
